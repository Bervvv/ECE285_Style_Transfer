{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data as td \n",
    "import torchvision as tv\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import cGAN_model.nntools as nt\n",
    "\n",
    "from itertools import chain\n",
    "from cGAN_model.DnCNN import DnCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class used for loading content for training or testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransDataset(td.Dataset):\n",
    "\n",
    "    def __init__(self, content_root_dir, style_root_dir,\n",
    "                 content_category, style_category,\n",
    "                 image_size=(150, 150), sigma=30): \n",
    "        super(StyleTransDataset, self).__init__()\n",
    "        self.content_cat = content_category\n",
    "        self.style_cat = style_category\n",
    "        self.image_size = image_size\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        self.content_dir = os.path.join(content_root_dir, content_category) \n",
    "        self.style_dir = os.path.join(style_root_dir, style_category) \n",
    "        self.content_files = os.listdir(self.content_dir)\n",
    "        self.style_files = os.listdir(self.style_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.content_files) \n",
    "    \n",
    "    def content_num(self):\n",
    "        return len(self.content_files)\n",
    "    \n",
    "    def style_num(self):\n",
    "        return len(self.style_files)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f\"StyleTransDataset(category: {self.content_cat}\"\n",
    "                f\", image_size={self.image_size}, sigma={self.sigma})\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        content_path = os.path.join(self.content_dir, self.content_files[idx]) \n",
    "        content = Image.open(content_path).convert('RGB')\n",
    "          \n",
    "        return self.trans(content)\n",
    "    \n",
    "    def trans(self, img):\n",
    "        i = np.random.randint(img.size[0] - self.image_size[0]) \n",
    "        j = np.random.randint(img.size[1] - self.image_size[1]) \n",
    "\n",
    "        img = img.crop([i, j , i + self.image_size[0], j + \n",
    "                            self.image_size[1]]) \n",
    "        \n",
    "        transform = tv.transforms.Compose([\n",
    "            tv.transforms.ToTensor(),\n",
    "            tv.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "            ])\n",
    "        \n",
    "        img = transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|    Sytle Catogries1    |      Sytle Catogries2      |  Sytle Catogries3  | Content Catogries |\n",
    "|:----------------------:|:--------------------------:|:------------------:|:-----------------:|\n",
    "| Abstract_Expressionism | Expressionism              | Pointillism        | city              |\n",
    "| Action_painting        | Fauvism                    | Pop_Art            | field             |\n",
    "| Analytical_Cubism      | High_Renaissance           | Post_Impressionism | forest            |\n",
    "| Art_Nouveau_Modern     | Impressionism              | Realism            | lake              |\n",
    "| Baroque                | Mannerism_Late_Renaissance | Rococo             | licenses          |\n",
    "| Color_Field_Painting   | Minimalism                 | Romanticism        | mountain          |\n",
    "| Contemporary_Realism   | Naive_Art_Primitivism      | Symbolism          | ocean             |\n",
    "| Cubism                 | New_Realism                | Synthetic_Cubism   | road              |\n",
    "| Early_Renaissance      | Northern_Renaissance       | Ukiyo_e            |                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_root_dir = \"//datasets/ee285f-public/flickr_landscape/\"\n",
    "style_root_dir = \"/datasets/ee285f-public/wikiart/wikiart/\"\n",
    "train_set = StyleTransDataset(content_root_dir, style_root_dir, \"city\", \"Art_Nouveau_Modern\")\n",
    "\n",
    "large_train_set = train_set\n",
    "for cat in [\"forest\", \"road\"]:\n",
    "    large_train_set += StyleTransDataset(content_root_dir, style_root_dir, cat, \"Art_Nouveau_Modern\")\n",
    "\n",
    "test_set = StyleTransDataset(content_root_dir, style_root_dir, \"mountain\", \"Art_Nouveau_Modern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get style reference (starry night.jpg) for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tv.transforms.Compose([\n",
    "            tv.transforms.Resize((150, 150)),\n",
    "            tv.transforms.ToTensor(),\n",
    "            tv.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "            ])\n",
    "\n",
    "star_night_style_ref = Image.open(\"./starry_night.jpg\").convert('RGB')\n",
    "star_night_style_ref = transform(star_night_style_ref)\n",
    "\n",
    "# use for plot and see effect\n",
    "content_ref = Image.open(\"./house.jpg\").convert('RGB')\n",
    "content_ref = transform(content_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myimshow(image, ax=plt):\n",
    "    image = image.to('cpu').numpy()\n",
    "    image = np.moveaxis(image, [0, 1, 2], [2, 0, 1]) \n",
    "    image = (image + 1) / 2\n",
    "    image[image < 0] = 0\n",
    "    image[image > 1] = 1 \n",
    "    h = ax.imshow(image) \n",
    "    ax.axis('off') \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator and Discriminator for GAN\n",
    "\n",
    "### Generator use DnCNN from HW4, with MSE loss\n",
    "\n",
    "### Discriminator use 4 layers of CNN (3 -> 16, 64, 256, 256) along with instNorm and RELU for transforming the image. Then, it use CNN(256 -> 1) for discriminating decision making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(DnCNN):\n",
    "    def __init__(self, D, C=64):\n",
    "        super(Generator, self).__init__(D)\n",
    "        \n",
    "class Discriminator(nt.NeuralNetwork):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential()\n",
    "        dims = [3, 16, 64, 256, 256]\n",
    "        for i in range(len(dims) - 1):\n",
    "            self.cnn.add_module(f\"conv2d{i}\", nn.Conv2d(dims[i], dims[i + 1], 3, padding = 1))\n",
    "            self.cnn.add_module(f\"instNorm{i}\", nn.InstanceNorm2d(dims[i + 1]))\n",
    "            self.cnn.add_module(f\"relu{i}\", nn.LeakyReLU(0.2, True))\n",
    "        \n",
    "        self.cnn.add_module(\"cov2dFL\", nn.Conv2d(256, 1, 3, padding=1, bias=False))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.cnn(x)\n",
    "        h = h.view(h.size(0), -1) # reshape to 2D (batch_size * (img_w * img*h))\n",
    "        return h\n",
    "    \n",
    "    def criterion(self, y, d):\n",
    "        return nn.L1Loss()(y, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer class used to train cycle GAN, geting loss/gradient for generator and discriminator and do optimization with Adam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGANTrainer():\n",
    "\n",
    "    def __init__(self, gen2s, gen2c, dis_c, dis_s, device):\n",
    "        self.device = device\n",
    "        \n",
    "        self.gen2s = gen2s\n",
    "        self.gen2c = gen2c\n",
    "        self.dis_c = dis_c\n",
    "        self.dis_s = dis_s\n",
    "        \n",
    "        self.lr = 1e-3\n",
    "        self.adam_gen = torch.optim.Adam(chain(gen2s.parameters(), gen2c.parameters()), lr=self.lr, betas=(0.5,0.999))\n",
    "        self.adam_dis_c = torch.optim.Adam(dis_c.parameters(), lr=self.lr, betas=(0.5,0.999))\n",
    "        self.adam_dis_s = torch.optim.Adam(dis_s.parameters(), lr=self.lr, betas=(0.5,0.999))\n",
    "        \n",
    "        self.l1Loss = nn.L1Loss().to(self.device)\n",
    "        self.l2Loss = nn.MSELoss().to(self.device)\n",
    "        \n",
    "    def forward(self, content, style):\n",
    "        self.content = content\n",
    "        self.style = style\n",
    "        self.S_c = self.gen2s(content)\n",
    "        self.C_S_c = self.gen2c(self.S_c)\n",
    "        self.C_s = self.gen2c(style)\n",
    "        self.S_C_s = self.gen2s(self.C_s)\n",
    "        \n",
    "    def train_generator(self):  \n",
    "        self.adam_gen.zero_grad()\n",
    "\n",
    "        totalLoss = 0\n",
    "\n",
    "        # get Discriminator Loss\n",
    "        disS= self.dis_s(self.S_c)\n",
    "        real_var = Variable(torch.cuda.FloatTensor(disS.shape).fill_(1.0),\n",
    "                            requires_grad = False)\n",
    "        totalLoss += self.l2Loss(disS, real_var)\n",
    "        \n",
    "        disC = self.dis_c(self.C_s)\n",
    "        real_var = Variable(torch.cuda.FloatTensor(disC.shape).fill_(1.0),\n",
    "                            requires_grad = False)\n",
    "        totalLoss += self.l2Loss(disC, real_var)\n",
    "\n",
    "        # get Cycle GAN Loss\n",
    "        totalLoss += self.l1Loss(self.C_S_c, self.content)\n",
    "        totalLoss += self.l1Loss(self.S_C_s, self.style)\n",
    "\n",
    "        # update generator\n",
    "        totalLoss.backward()\n",
    "        self.adam_gen.step()\n",
    "        return totalLoss\n",
    "    \n",
    "    def train_discriminator(self, mode):\n",
    "        \"\"\"\n",
    "        Train the discriminator. \n",
    "        mode == 0: input content, train the discriminator for style \n",
    "        mode == 1: input style, train the discriminator for content\n",
    "        ref: real picture to distinguish from\n",
    "        \"\"\"\n",
    "\n",
    "        assert (mode == 0 or mode == 1), \"input must be 0(in: content) or 1(in: style)\" \n",
    "\n",
    "        if mode == 0:\n",
    "            # Train the discrimination for style\n",
    "            # Input a content file, and generate a fake style\n",
    "            \n",
    "            adam_dis = self.adam_dis_s\n",
    "            dis, gen, ori = self.dis_s, self.S_c, self.style\n",
    "        else:\n",
    "            # Train the discrimination for content\n",
    "            # Input a style file, and generate a fake content\n",
    "            adam_dis = self.adam_dis_c\n",
    "            dis, gen, ori = self.dis_c, self.C_s, self.content\n",
    "        \n",
    "            \n",
    "        adam_dis.zero_grad()   \n",
    "        \n",
    "        totalLoss = 0\n",
    "        \n",
    "        disReal = dis(ori)\n",
    "        real_var = Variable(torch.cuda.FloatTensor(disReal.shape).fill_(1.0),\n",
    "                            requires_grad = False)\n",
    "        totalLoss += self.l2Loss(disReal, real_var)\n",
    "        \n",
    "        # get Discriminator Loss\n",
    "        dis_fake = dis(gen.detach())\n",
    "        fake_var = Variable(torch.cuda.FloatTensor(dis_fake.shape).fill_(0.0),\n",
    "                            requires_grad = False)\n",
    "\n",
    "        totalLoss += self.l2Loss(dis_fake, fake_var)\n",
    "        \n",
    "        # update discriminator\n",
    "        totalLoss.backward()\n",
    "        adam_dis.step()\n",
    "        return totalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment inherited from nt.Experiment. Being able to run experiment, save/load check points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGANexp(nt.Experiment):\n",
    "    def __init__(self, cGANTrainer, train_set, output_dir, style_ref,\n",
    "                 picNum = 100, batch_size=16, device = device,\n",
    "                 perform_validation_during_training=False):  \n",
    "        # Initialize\n",
    "        self.history = []\n",
    "        self.trainer = cGANTrainer\n",
    "        self.device = device\n",
    "        self.train_set = train_set\n",
    "        self.style_ref = style_ref\n",
    "        \n",
    "        self.picNum = picNum\n",
    "        self.net = self.trainer.gen2c\n",
    "\n",
    "        self.test_loader = td.DataLoader(train_set,\n",
    "                  batch_size=4, shuffle=False, \n",
    "                  drop_last=True, pin_memory=True)\n",
    "                \n",
    "        self.toRecover = {\n",
    "            'contentGenNet': self.trainer.gen2c,\n",
    "            'styleGenNet': self.trainer.gen2s,\n",
    "            'contentDisNet': self.trainer.dis_c,\n",
    "            'styleDisNet': self.trainer.dis_s,\n",
    "            'genAdam': self.trainer.adam_gen,\n",
    "            'contentDisAdam': self.trainer.adam_dis_c,\n",
    "            'styleDisAdam': self.trainer.adam_dis_s,\n",
    "            'history': self.history\n",
    "           }\n",
    "        \n",
    "        # Define checkpoint paths\n",
    "        if output_dir is None:\n",
    "            output_dir = 'experiment_{}'.format(time.time())\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.checkpoint_path = os.path.join(output_dir, \"checkpoint.pth.tar\")\n",
    "        self.config_path = os.path.join(output_dir, \"config.txt\")\n",
    "\n",
    "        # Transfer all local arguments/variables into attributes\n",
    "        locs = {k: v for k, v in locals().items() if k is not 'self'}\n",
    "        self.__dict__.update(locs)\n",
    "\n",
    "        # Load checkpoint and check compatibility\n",
    "        if os.path.isfile(self.config_path):\n",
    "            with open(self.config_path, 'r') as f:\n",
    "                if f.read()[:-1] != repr(self):\n",
    "                    raise ValueError(\n",
    "                        \"Cannot create this experiment: \"\n",
    "                        \"I found a checkpoint conflicting with the current setting.\")\n",
    "            print(\"Done Load from Checkpoint!\")\n",
    "            self.load()\n",
    "        else:\n",
    "            self.save()\n",
    "        \n",
    "    def setting(self):\n",
    "        \"\"\"Returns the setting of the experiment.\"\"\"\n",
    "        return {'contentGenNet': self.trainer.gen2c,\n",
    "                'styleGenNet': self.trainer.gen2s,\n",
    "                'contentDisNet': self.trainer.dis_c,\n",
    "                'styleDisNet': self.trainer.dis_s,\n",
    "                'genAdam': self.trainer.adam_gen,\n",
    "                'contentDisAdam': self.trainer.adam_dis_c,\n",
    "                'styleDisAdam': self.trainer.adam_dis_s,\n",
    "                'BatchSize': self.batch_size,\n",
    "                'PerformValidationDuringTraining': self.perform_validation_during_training}\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Pretty printer showing the setting of the experiment. This is what\n",
    "        is displayed when doing ``print(experiment)``. This is also what is\n",
    "        saved in the ``config.txt`` file.\n",
    "        \"\"\"\n",
    "        string = ''\n",
    "        for key, val in self.setting().items():\n",
    "            string += '{}({})\\n'.format(key, val)\n",
    "        return string\n",
    "    \n",
    "    def state_dict(self):\n",
    "        \"\"\"Returns the current state of the experiment.\"\"\"\n",
    "        return {'contentGenNet': self.trainer.gen2c.state_dict(),\n",
    "                'styleGenNet': self.trainer.gen2s.state_dict(),\n",
    "                'contentDisNet': self.trainer.dis_c.state_dict(),\n",
    "                'styleDisNet': self.trainer.dis_s.state_dict(),\n",
    "                'genAdam': self.trainer.adam_gen.state_dict(),\n",
    "                'contentDisAdam': self.trainer.adam_dis_c.state_dict(),\n",
    "                'styleDisAdam': self.trainer.adam_dis_s.state_dict(),\n",
    "                'history': self.history\n",
    "               }\n",
    "    \n",
    "    \n",
    "    def load_state_dict(self, checkpoint):\n",
    "        \"\"\"Loads the experiment from the input checkpoint.\"\"\"\n",
    "        for key, val in checkpoint.items():\n",
    "            if key not in self.toRecover:\n",
    "                raise AttributeError(f\"Loading is Wrong! Key is {key}\")\n",
    "            if key == 'history':\n",
    "                self.history = val\n",
    "            else:\n",
    "                self.toRecover[key].load_state_dict(val)\n",
    "               \n",
    "        nets = [self.trainer.gen2c, self.trainer.gen2s, \n",
    "                self.trainer.dis_c, self.trainer.dis_s]\n",
    "        adams = [self.trainer.adam_gen, self.trainer.adam_gen,\n",
    "                 self.trainer.adam_dis_c, self.trainer.adam_dis_s]\n",
    "        \n",
    "        for net, optimizer in zip(nets, adams):\n",
    "            for state in optimizer.state.values():\n",
    "                for k, v in state.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        state[k] = v.to(self.device)\n",
    "                        \n",
    "    def load(self):\n",
    "        \"\"\"Loads the experiment from the last checkpoint saved on disk.\"\"\"\n",
    "        checkpoint = torch.load(self.checkpoint_path,\n",
    "                                map_location=self.device)\n",
    "        self.load_state_dict(checkpoint)\n",
    "        del checkpoint\n",
    "        \n",
    "    def save(self):\n",
    "        \"\"\"Saves the experiment on disk, i.e, create/update the last checkpoint.\"\"\"\n",
    "        torch.save(self.state_dict(), self.checkpoint_path)\n",
    "        with open(self.config_path, 'w') as f:\n",
    "            print(self, file=f)\n",
    "        \n",
    "    def run(self, num_epochs, plot=None):\n",
    "        style_ref = self.style_ref\n",
    "        start_epoch = self.epoch\n",
    "        print(\"Start/Continue training from epoch {}\".format(start_epoch))\n",
    "        if plot is not None:\n",
    "            plot(self)\n",
    "                \n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            s = time.time()\n",
    "            i = 0\n",
    "            gen_loss = []\n",
    "            dis_c_loss = []\n",
    "            dis_s_loss = []\n",
    "            total_loss = []\n",
    "            \n",
    "            for content in self.test_loader:\n",
    "                if i > self.picNum:\n",
    "                    break\n",
    "                content = content.to(self.device)\n",
    "                style = style_ref[np.newaxis].to(self.device)\n",
    "                \n",
    "                self.trainer.forward(content, style)\n",
    "                \n",
    "                # train generator\n",
    "                gen_loss.append(self.trainer.train_generator().item())\n",
    "                \n",
    "                # train discrimiator\n",
    "                dis_c_loss.append(self.trainer.train_discriminator(0).item())\n",
    "                dis_s_loss.append(self.trainer.train_discriminator(1).item())\n",
    "            \n",
    "                i += 1\n",
    "                \n",
    "            self.history.append((np.mean(gen_loss+dis_c_loss+dis_s_loss), np.mean(gen_loss), np.mean(dis_c_loss+dis_s_loss)))\n",
    "            print(\"Epoch {} (Time: {:.2f}s)\".format(\n",
    "                self.epoch, time.time() - s))\n",
    "            self.save()\n",
    "            if plot is not None:\n",
    "                plot(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(exp, fig, axes, content, style, visu_rate=2): \n",
    "    if exp.epoch % visu_rate != 0:\n",
    "        return\n",
    "    with torch.no_grad():\n",
    "        transfered = exp.trainer.gen2s(content[np.newaxis].to(exp.trainer.gen2s.device))[0] \n",
    "    axes[0][0].clear()\n",
    "    axes[0][1].clear()\n",
    "    axes[1][0].clear()\n",
    "    axes[1][1].clear()\n",
    "    myimshow(content, ax=axes[0][0]) \n",
    "    axes[0][0].set_title('Content image') \n",
    "    \n",
    "    myimshow(style, ax=axes[0][1]) \n",
    "    axes[0][1].set_title('Style image')\n",
    "\n",
    "    myimshow(transfered, ax=axes[1][0]) \n",
    "    axes[1][0].set_title('Transfered image')\n",
    "    \n",
    "    axes[1][1].plot([exp.history[k][0].item() \n",
    "                     for k in range(exp.epoch)],label=\"Total Loss\")\n",
    "    axes[1][1].plot([exp.history[k][1].item() \n",
    "                     for k in range(exp.epoch)],label=\"Gen Loss\")\n",
    "    axes[1][1].plot([exp.history[k][2].item()\n",
    "                     for k in range(exp.epoch)],label=\"Dis Loss\")\n",
    "    \n",
    "    axes[1][1].legend(loc='best')\n",
    "    \n",
    "    axes[1][1].set_xlabel(\"Epoch\")\n",
    "    axes[1][1].set_ylabel(\"Loss\")\n",
    "    \n",
    "    plt.tight_layout() \n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init networks & experiment for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen2s_short = Generator(6).to(device)\n",
    "gen2c_short = Generator(6).to(device)\n",
    "dis_c_short = Discriminator().to(device)\n",
    "dis_s_short = Discriminator().to(device)\n",
    "\n",
    "gen2s_long = Generator(6).to(device)\n",
    "gen2c_long = Generator(6).to(device)\n",
    "dis_c_long = Discriminator().to(device)\n",
    "dis_s_long = Discriminator().to(device)\n",
    "\n",
    "cycleGan_trainer_short = CGANTrainer(gen2s_short, gen2c_short, dis_c_short, dis_s_short, device)\n",
    "cycleGan_trainer_long = CGANTrainer(gen2s_long, gen2c_long, dis_c_long, dis_s_long, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cycleGAN_exp_short = CGANexp(cycleGan_trainer_short, train_set, output_dir=\"./cGAN_ckpts/cycleGAN_ckpt\", \n",
    "                             style_ref = star_night_style_ref, batch_size = 4, picNum = 20, \n",
    "                             perform_validation_during_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cycleGAN_exp_long = CGANexp(cycleGan_trainer_long, large_train_set, output_dir=\"./cGAN_ckpts/cycleGAN_ckpt_long\", \n",
    "                       style_ref = star_night_style_ref, batch_size = 2, picNum = 1000, \n",
    "                       perform_validation_during_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use style_ref and content_ref for plotting transfered image and loss lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(7,6)) \n",
    "cycleGAN_exp_short.run(num_epochs=200, plot=lambda exp: plot(exp, fig=fig, axes=axes, \n",
    "                                                content=content_ref,\n",
    "                                                style=star_night_style_ref))\n",
    "# fig.savefig(\"./proj_report_img/exp_short_train_wTotalLoss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(7,6)) \n",
    "cycleGAN_exp_long.run(num_epochs=200, plot=lambda exp: plot(exp, fig=fig, axes=axes, \n",
    "                                                content=content_ref,\n",
    "                                                style=star_night_style_ref, visu_rate=1))\n",
    "# fig.savefig(\"./proj_report_img/exp_long_long_train_wTotalLoss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_final_result(save_path, use_train, *test_ids):\n",
    "    assert (type(save_path)==str), \"first input must be string for save path\" \n",
    "    assert (type(use_train)==bool), \"second input must be True or False\" \n",
    "    img_num = len(test_ids)\n",
    "    fig, axes = plt.subplots(ncols=4, nrows=img_num, figsize=(7,6)) \n",
    "    \n",
    "    for i in range(img_num):\n",
    "        if use_train:\n",
    "            to_test = train_set[test_ids[i]][np.newaxis].to(device)\n",
    "        else:\n",
    "            to_test = test_set[test_ids[i]][np.newaxis].to(device)\n",
    "            \n",
    "        myimshow(to_test[0], axes[i][0])\n",
    "        myimshow(star_night_style_ref, axes[i][1])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            myimshow(cycleGAN_exp_short.trainer.gen2s(to_test)[0] , ax=axes[i][2])\n",
    "            myimshow(cycleGAN_exp_long.trainer.gen2s(to_test)[0] , ax=axes[i][3])\n",
    "\n",
    "        axes[i][0].set_title(\"content\")\n",
    "        axes[i][1].set_title(\"style\")\n",
    "        axes[i][2].set_title(\"exp_short\")\n",
    "        axes[i][3].set_title(\"exp_long\")\n",
    "    fig.tight_layout()\n",
    "#     fig.savefig(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_final_result(\"./proj_report_img/multi_test_result.png\", True, 2, 40, 180, 230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
