{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data as td \n",
    "import torchvision as tv\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "import nntools as nt\n",
    "\n",
    "from itertools import chain\n",
    "from DnCNN import DnCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransDataset(td.Dataset):\n",
    "\n",
    "    def __init__(self, content_root_dir, style_root_dir,\n",
    "                 content_category, style_category,\n",
    "                 image_size=(150, 150), sigma=30): \n",
    "        super(StyleTransDataset, self).__init__()\n",
    "        self.content_cat = content_category\n",
    "        self.style_cat = style_category\n",
    "        self.image_size = image_size\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        self.content_dir = os.path.join(content_root_dir, content_category) \n",
    "        self.style_dir = os.path.join(style_root_dir, style_category) \n",
    "        self.content_files = os.listdir(self.content_dir)\n",
    "        self.style_files = os.listdir(self.style_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.content_files) #min(self.content_num(), self.style_num())\n",
    "    \n",
    "    def content_num(self):\n",
    "        return len(self.content_files)\n",
    "    \n",
    "    def style_num(self):\n",
    "        return len(self.style_files)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f\"StyleTransDataset(category: {self.content_cat}\"\n",
    "                f\", image_size={self.image_size}, sigma={self.sigma})\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        content_path = os.path.join(self.content_dir, self.content_files[idx]) \n",
    "        content = Image.open(content_path).convert('RGB')\n",
    "        \n",
    "#         style_path = os.path.join(self.style_dir, self.style_files[idx]) \n",
    "#         style = Image.open(style_path).convert('RGB')\n",
    "        \n",
    "        return self.trans(content) #, self.trans(style)\n",
    "    \n",
    "    def trans(self, img):\n",
    "        i = np.random.randint(img.size[0] - self.image_size[0]) \n",
    "        j = np.random.randint(img.size[1] - self.image_size[1]) \n",
    "\n",
    "        img = img.crop([i, j , i + self.image_size[0], j + \n",
    "                            self.image_size[1]]) \n",
    "        \n",
    "        transform = tv.transforms.Compose([\n",
    "            tv.transforms.ToTensor(),\n",
    "            tv.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "            ])\n",
    "        \n",
    "        img = transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Style**:\n",
    "Abstract_Expressionism\t    Minimalism\n",
    "Action_painting\t\t    Naive_Art_Primitivism\n",
    "Analytical_Cubism\t    New_Realism\n",
    "Art_Nouveau_Modern\t    Northern_Renaissance\n",
    "Baroque\t\t\t    Pointillism\n",
    "Color_Field_Painting\t    Pop_Art\n",
    "Contemporary_Realism\t    Post_Impressionism\n",
    "Cubism\t\t\t    Realism\n",
    "Early_Renaissance\t    Rococo\n",
    "Expressionism\t\t    Romanticism\n",
    "Fauvism\t\t\t    Symbolism\n",
    "High_Renaissance\t    Synthetic_Cubism\n",
    "Impressionism\t\t    Ukiyo_e\n",
    "Mannerism_Late_Renaissance\n",
    "\n",
    "**Content:**\n",
    "city   forest  licenses  ocean  road\n",
    "field  lake    mountain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_root_dir = \"//datasets/ee285f-public/flickr_landscape/\"\n",
    "style_root_dir = \"/datasets/ee285f-public/wikiart/wikiart/\"\n",
    "train_set = StyleTransDataset(content_root_dir, style_root_dir, \"city\", \"Art_Nouveau_Modern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_ref = Image.open(\"./starry_night.jpg\").convert('RGB')\n",
    "transform = tv.transforms.Compose([\n",
    "            tv.transforms.Resize((150, 150)),\n",
    "            tv.transforms.ToTensor(),\n",
    "            tv.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "            ])\n",
    "style_ref = transform(style_ref)\n",
    "\n",
    "content_ref = Image.open(\"./house.jpg\").convert('RGB')\n",
    "transform = tv.transforms.Compose([\n",
    "            tv.transforms.Resize((150, 150)),\n",
    "            tv.transforms.ToTensor(),\n",
    "            tv.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "            ])\n",
    "content_ref = transform(content_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train_set[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myimshow(image, ax=plt):\n",
    "    image = image.to('cpu').numpy()\n",
    "    image = np.moveaxis(image, [0, 1, 2], [2, 0, 1]) \n",
    "    image = (image + 1) / 2\n",
    "    image[image < 0] = 0\n",
    "    image[image > 1] = 1 \n",
    "    h = ax.imshow(image) \n",
    "    ax.axis('off') \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(DnCNN):\n",
    "    def __init__(self, D, C=64):\n",
    "        super(Generator, self).__init__(D)\n",
    "        \n",
    "    def criterion(self, y, d):\n",
    "        return nn.MSELoss()(y, d)\n",
    "        \n",
    "class Discriminator():\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential()\n",
    "        dims = [3, 16, 64, 256, 256]\n",
    "        for i in range(len(dims) - 1):\n",
    "            self.cnn.add_module(f\"conv2d{i}\", nn.Conv2d(dims[i], dims[i + 1], 3, padding = 1))\n",
    "            self.cnn.add_module(f\"instNorm{i}\", nn.InstanceNorm2d(dims[i + 1]))\n",
    "            self.cnn.add_module(f\"relu{i}\", nn.LeakyReLU(0.2, True))\n",
    "        \n",
    "        self.cnn.add_module(\"cov2dFL\", nn.Conv2d(256, 1, 3, padding=1, bias=False))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.cnn(x)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        return h\n",
    "    \n",
    "    def criterion(self, y, d):\n",
    "        return nn.L1Loss()(y, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGANTrainer():\n",
    "\n",
    "    def __init__(self, gen2s, gen2c, dis_c, dis_s, device):\n",
    "        self.device = device\n",
    "        \n",
    "        self.gen2s = gen2s\n",
    "        self.gen2c = gen2c\n",
    "        self.dis_c = dis_c\n",
    "        self.dis_s = dis_s\n",
    "        \n",
    "        self.lr = 1e-3\n",
    "        self.adam_gen = torch.optim.Adam(chain(gen2s.parameters(), gen2c.parameters()), lr=self.lr, betas=(0.5,0.999))\n",
    "        self.adam_dis_c = torch.optim.Adam(dis_c.parameters(), lr=self.lr, betas=(0.5,0.999))\n",
    "        self.adam_dis_s = torch.optim.Adam(dis_s.parameters(), lr=self.lr, betas=(0.5,0.999))\n",
    "        \n",
    "        self.l1Loss = nn.L1Loss().to(self.device)\n",
    "        self.l2Loss = nn.MSELoss().to(self.device)\n",
    "        \n",
    "    def forward(self, content, style):\n",
    "        self.content = content\n",
    "        self.style = style\n",
    "        self.S_c = self.gen2s(content)\n",
    "        self.C_S_c = self.gen2c(self.S_c)\n",
    "        self.C_s = self.gen2c(style)\n",
    "        self.S_C_s = self.gen2s(self.C_s)\n",
    "        \n",
    "    def train_generator(self):  \n",
    "        self.adam_gen.zero_grad()\n",
    "\n",
    "        totalLoss = 0\n",
    "\n",
    "        # get identity loss for same input same output            \n",
    "#         gen_style = self.gen2s(self.style)\n",
    "#         totalLoss += self.l1Loss(gen_style, self.style)\n",
    "        \n",
    "#         gen_content = self.gen2c(self.content)\n",
    "#         totalLoss += self.l1Loss(gen_content, self.content)\n",
    "\n",
    "        # get Discriminator Loss\n",
    "        disS= self.dis_s(self.S_c)\n",
    "        real_var = Variable(torch.cuda.FloatTensor(disS.shape).fill_(1.0),\n",
    "                            requires_grad = False)\n",
    "        totalLoss += self.l2Loss(disS, real_var)\n",
    "        \n",
    "        disC = self.dis_c(self.C_s)\n",
    "        real_var = Variable(torch.cuda.FloatTensor(disC.shape).fill_(1.0),\n",
    "                            requires_grad = False)\n",
    "        totalLoss += self.l2Loss(disC, real_var)\n",
    "\n",
    "        # get Cycle GAN Loss\n",
    "        totalLoss += self.l1Loss(self.C_S_c, self.content)\n",
    "        totalLoss += self.l1Loss(self.S_C_s, self.style)\n",
    "\n",
    "        # update generator\n",
    "        totalLoss.backward()\n",
    "        self.adam_gen.step()\n",
    "        return totalLoss\n",
    "    \n",
    "    def train_discriminator(self, mode):\n",
    "        \"\"\"\n",
    "        Train the discriminator. \n",
    "        mode == 0: input content, train the discriminator for style \n",
    "        mode == 1: input style, train the discriminator for content\n",
    "        ref: real picture to distinguish from\n",
    "        \"\"\"\n",
    "\n",
    "        assert (mode == 0 or mode == 1), \"input must be 0(in: content) or 1(in: style)\" \n",
    "\n",
    "        if mode == 0:\n",
    "            # Train the discrimination for style\n",
    "            # Input a content file, and generate a fake style\n",
    "            \n",
    "            adam_dis = self.adam_dis_s\n",
    "            dis, gen, ori = self.dis_s, self.S_c, self.style #self.gen2s\n",
    "        else:\n",
    "            # Train the discrimination for content\n",
    "            # Input a style file, and generate a fake content\n",
    "            adam_dis = self.adam_dis_c\n",
    "            dis, gen, ori = self.dis_c, self.C_s, self.content #self.gen2c\n",
    "        \n",
    "            \n",
    "        adam_dis.zero_grad()   \n",
    "        \n",
    "        totalLoss = 0\n",
    "        \n",
    "        disReal = dis(ori)\n",
    "        real_var = Variable(torch.cuda.FloatTensor(disReal.shape).fill_(1.0),\n",
    "                            requires_grad = False)\n",
    "        totalLoss += self.l2Loss(disReal, real_var)\n",
    "        \n",
    "        # get Discriminator Loss\n",
    "        dis_fake = dis(gen.detach())\n",
    "        fake_var = Variable(torch.cuda.FloatTensor(dis_fake.shape).fill_(0.0),\n",
    "                            requires_grad = False)\n",
    "\n",
    "        totalLoss += self.l2Loss(dis_fake, fake_var)\n",
    "        \n",
    "        # update discriminator\n",
    "        totalLoss.backward()\n",
    "        adam_dis.step()\n",
    "        return totalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGANexp(nt.Experiment):\n",
    "    def __init__(self, cGANTrainer, train_set, output_dir, \n",
    "                 picNum = 100, batch_size=16, device = device,\n",
    "                 perform_validation_during_training=False):  \n",
    "        # Initialize\n",
    "        self.history = []\n",
    "        self.trainer = cGANTrainer\n",
    "        self.device = device\n",
    "        \n",
    "        self.criterionIdt = torch.nn.L1Loss()\n",
    "        self.criterionIdt = torch.nn.L1Loss()\n",
    "        self.criterionIdt = torch.nn.L1Loss()\n",
    "        \n",
    "        self.picNum = picNum\n",
    "        self.net = self.trainer.gen2c\n",
    "\n",
    "        self.test_loader = td.DataLoader(train_set,\n",
    "                  batch_size=4, shuffle=False, \n",
    "                  drop_last=True, pin_memory=True)\n",
    "                \n",
    "        self.toRecover = {\n",
    "            'contentGenNet': self.trainer.gen2c,\n",
    "            'styleGenNet': self.trainer.gen2s,\n",
    "            'contentDisNet': self.trainer.dis_c,\n",
    "            'styleDisNet': self.trainer.dis_s,\n",
    "            'genAdam': self.trainer.adam_gen,\n",
    "            'contentDisAdam': self.trainer.adam_dis_c,\n",
    "            'styleDisAdam': self.trainer.adam_dis_s,\n",
    "            'history': self.history\n",
    "           }\n",
    "        \n",
    "        # Define checkpoint paths\n",
    "        if output_dir is None:\n",
    "            output_dir = 'experiment_{}'.format(time.time())\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.checkpoint_path = os.path.join(output_dir, \"checkpoint.pth.tar\")\n",
    "        self.config_path = os.path.join(output_dir, \"config.txt\")\n",
    "\n",
    "        # Transfer all local arguments/variables into attributes\n",
    "        locs = {k: v for k, v in locals().items() if k is not 'self'}\n",
    "        self.__dict__.update(locs)\n",
    "\n",
    "        # Load checkpoint and check compatibility\n",
    "        if os.path.isfile(self.config_path):\n",
    "            with open(self.config_path, 'r') as f:\n",
    "                if f.read()[:-1] != repr(self):\n",
    "                    raise ValueError(\n",
    "                        \"Cannot create this experiment: \"\n",
    "                        \"I found a checkpoint conflicting with the current setting.\")\n",
    "            print(\"Done Load from Checkpoint!\")\n",
    "            self.load()\n",
    "        else:\n",
    "            self.save()\n",
    "        \n",
    "    def setting(self):\n",
    "        \"\"\"Returns the setting of the experiment.\"\"\"\n",
    "        return {'contentGenNet': self.trainer.gen2c,\n",
    "                'TrainSet': self.train_set,\n",
    "                'styleGenNet': self.trainer.gen2s,\n",
    "                'contentDisNet': self.trainer.dis_c,\n",
    "                'styleDisNet': self.trainer.dis_s,\n",
    "                'genAdam': self.trainer.adam_gen,\n",
    "                'contentDisAdam': self.trainer.adam_dis_c,\n",
    "                'styleDisAdam': self.trainer.adam_dis_s,\n",
    "                'TrainSet': self.train_set,\n",
    "                'BatchSize': self.batch_size,\n",
    "                'PerformValidationDuringTraining': self.perform_validation_during_training}\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Pretty printer showing the setting of the experiment. This is what\n",
    "        is displayed when doing ``print(experiment)``. This is also what is\n",
    "        saved in the ``config.txt`` file.\n",
    "        \"\"\"\n",
    "        string = ''\n",
    "        for key, val in self.setting().items():\n",
    "            string += '{}({})\\n'.format(key, val)\n",
    "        return string\n",
    "    \n",
    "    def state_dict(self):\n",
    "        \"\"\"Returns the current state of the experiment.\"\"\"\n",
    "        return {'contentGenNet': self.trainer.gen2c.state_dict(),\n",
    "                'styleGenNet': self.trainer.gen2s.state_dict(),\n",
    "                'contentDisNet': self.trainer.dis_c.state_dict(),\n",
    "                'styleDisNet': self.trainer.dis_s.state_dict(),\n",
    "                'genAdam': self.trainer.adam_gen.state_dict(),\n",
    "                'contentDisAdam': self.trainer.adam_dis_c.state_dict(),\n",
    "                'styleDisAdam': self.trainer.adam_dis_s.state_dict(),\n",
    "                'history': self.history\n",
    "               }\n",
    "    \n",
    "    \n",
    "    def load_state_dict(self, checkpoint):\n",
    "        \"\"\"Loads the experiment from the input checkpoint.\"\"\"\n",
    "        for key, val in checkpoint.items():\n",
    "            if key not in self.toRecover:\n",
    "                raise AttributeError(f\"Loading is Wrong! Key is {key}\")\n",
    "            if key == 'history':\n",
    "                self.history = val\n",
    "            else:\n",
    "                self.toRecover[key].load_state_dict(val)\n",
    "               \n",
    "        nets = [self.trainer.gen2c, self.trainer.gen2s, \n",
    "                self.trainer.dis_c, self.trainer.dis_s]\n",
    "        adams = [self.trainer.adam_gen, self.trainer.adam_gen,\n",
    "                 self.trainer.adam_dis_c, self.trainer.adam_dis_s]\n",
    "        \n",
    "        for net, optimizer in zip(nets, adams):\n",
    "            for state in optimizer.state.values():\n",
    "                for k, v in state.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        state[k] = v.to(self.device)\n",
    "                        \n",
    "    def load(self):\n",
    "        \"\"\"Loads the experiment from the last checkpoint saved on disk.\"\"\"\n",
    "        checkpoint = torch.load(self.checkpoint_path,\n",
    "                                map_location=self.device)\n",
    "        self.load_state_dict(checkpoint)\n",
    "        del checkpoint\n",
    "        \n",
    "    def save(self):\n",
    "        \"\"\"Saves the experiment on disk, i.e, create/update the last checkpoint.\"\"\"\n",
    "        torch.save(self.state_dict(), self.checkpoint_path)\n",
    "        with open(self.config_path, 'w') as f:\n",
    "            print(self, file=f)\n",
    "        \n",
    "    def run(self, num_epochs, plot=None):\n",
    "        global style_ref\n",
    "        \n",
    "        start_epoch = self.epoch\n",
    "        print(\"Start/Continue training from epoch {}\".format(start_epoch))\n",
    "        if plot is not None:\n",
    "            plot(self)\n",
    "                \n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            s = time.time()\n",
    "            i = 0\n",
    "            gen_loss = []\n",
    "            dis_c_loss = []\n",
    "            dis_s_loss = []\n",
    "            total_loss = []\n",
    "            \n",
    "            for content in self.test_loader:\n",
    "                if i > self.picNum:\n",
    "                    break\n",
    "                content = content.to(self.device)\n",
    "                style = style_ref[np.newaxis].to(self.device)\n",
    "                \n",
    "                self.trainer.forward(content, style)\n",
    "                \n",
    "                gen_loss.append(self.trainer.train_generator().item())\n",
    "                dis_c_loss.append(self.trainer.train_discriminator(0).item())\n",
    "                dis_s_loss.append(self.trainer.train_discriminator(1).item())\n",
    "            \n",
    "                i += 1\n",
    "                \n",
    "            self.history.append((np.mean(gen_loss+dis_c_loss+dis_s_loss), np.mean(gen_loss), np.mean(dis_c_loss+dis_s_loss)))\n",
    "            print(\"Epoch {} (Time: {:.2f}s)\".format(\n",
    "                self.epoch, time.time() - s))\n",
    "            self.save()\n",
    "            if plot is not None:\n",
    "                plot(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(exp, fig, axes, content, style, visu_rate=2): \n",
    "    if exp.epoch % visu_rate != 0:\n",
    "        return\n",
    "    with torch.no_grad():\n",
    "        transfered = exp.trainer.gen2s(content[np.newaxis].to(exp.trainer.gen2s.device))[0] \n",
    "    axes[0][0].clear()\n",
    "    axes[0][1].clear()\n",
    "    axes[1][0].clear()\n",
    "    axes[1][1].clear()\n",
    "    myimshow(content, ax=axes[0][0]) \n",
    "    axes[0][0].set_title('Content image') \n",
    "    \n",
    "    myimshow(style, ax=axes[0][1]) \n",
    "    axes[0][1].set_title('Style image')\n",
    "\n",
    "    myimshow(transfered, ax=axes[1][0]) \n",
    "    axes[1][0].set_title('Transfered image')\n",
    "    \n",
    "\n",
    "#     axes[1][1].plot([exp.history[k][0].item() \n",
    "#                      for k in range(exp.epoch)],label=\"Total Loss\")\n",
    "    axes[1][1].plot([exp.history[k][1].item() \n",
    "                     for k in range(exp.epoch)],label=\"Gen Loss\")\n",
    "    axes[1][1].plot([exp.history[k][2].item()\n",
    "                     for k in range(exp.epoch)],label=\"Dis Loss\")\n",
    "    \n",
    "    axes[1][1].legend(loc='best')\n",
    "    \n",
    "    axes[1][1].set_xlabel(\"Epoch\")\n",
    "    axes[1][1].set_ylabel(\"Loss\")\n",
    "    \n",
    "    plt.tight_layout() \n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen2s = Generator(6).to(device)\n",
    "gen2c = Generator(6).to(device)\n",
    "dis_c = Discriminator().to(device)\n",
    "dis_s = Discriminator().to(device)\n",
    "\n",
    "cycleGan_trainer = CGANTrainer(gen2s, gen2c, dis_c, dis_s, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(7,6)) \n",
    "\n",
    "cycleEXP_long = CGANexp(cycleGan_trainer, train_set, output_dir=\"cycleGAN_ckpt_long\",\n",
    "                        batch_size = 2, picNum = 500, perform_validation_during_training=True)\n",
    "\n",
    "cycleEXP_long.run(num_epochs=200, plot=lambda exp: plot(exp, fig=fig, axes=axes, \n",
    "                                                content=content_ref,\n",
    "                                                style=style_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cycleGAN_exp = CGANexp(cycleGan_trainer, train_set, output_dir=\"cycleGAN_ckpt\", batch_size = 4, picNum = 20,\n",
    "                     perform_validation_during_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(7,6)) \n",
    "cycleGAN_exp.run(num_epochs=200, plot=lambda exp: plot(exp, fig=fig, axes=axes, \n",
    "                                                content=content_ref,\n",
    "                                                style=style_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 40\n",
    "fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(7,6)) \n",
    "myimshow(train_set[ind].to(device), axes[0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    myimshow(cycleEXP_long.trainer.gen2s(train_set[ind][np.newaxis].to(device))[0], axes[1])\n",
    "    myimshow(cycleGAN_exp.trainer.gen2s(train_set[ind][np.newaxis].to(device))[0], axes[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
